\documentclass{cap2024}
% Use \documentclass[french]{cap2024} if you want to load the french version of the template.
% Note that if you choose to use the french option, you might encounter some unexpected behaviours, for example if you are using colons (:) in your labels.
% Remove the french option if you want to load the english version of the template.

% The following packages are already loaded:
% authblk
% hyperref
% natbib - Use the option no-natbib to avoid loading it (\documentclass[no-natbib]{cap2024})
% geometry with the options width=17cm and height=24cm
% amsthm
% and some environments are already defined and automatically switch between french and english: theorem, definition, lemma, corollary, proposition.

\usepackage{fancyvrb}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{booktabs} % for better tables
\usepackage{cleveref}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{mathtools,amsthm} % Math
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[table]{xcolor}
\definecolor{lightgray}{RGB}{230,230,230}
%\renewcommand{\theta}{\uptheta}
\DeclareMathOperator*{\argmax}{arg\,max}
\title{Weighted majority vote using Shapley values in crowdsourcing}
\author[1]{Tanguy Lefort\thanks{tanguy.lefort@umontpellier.fr}}
\author[2]{Benjamin Charlier}
\author[3]{Alexis Joly}
\author[4]{Joseph Salmon}
\affil[1]{University of Montpellier, IMAG, CNRS, LIRMM, Inria}
\affil[2]{University of Montpellier, IMAG, CNRS}
\affil[3]{LIRMM, Inria}
\affil[4]{University of Montpellier, IMAG, CNRS, Institut Universitaire de France (IUF)}


\begin{document}
\maketitle

\begin{abstract}
  Crowdsourcing has emerged as a pivotal paradigm for harnessing collective intelligence to solve data annotation tasks. Effective label aggregation, crucial for leveraging the diverse judgments of contributors, remains a fundamental challenge in crowdsourcing systems. This paper introduces a novel label aggregation strategy based on Shapley values, a concept originating from cooperative game theory. By integrating Shapley values as worker weights into the Weighted Majority Vote label aggregation (WMV), our proposed framework aims to address the interpretability of weights assigned to workers. This aggregation reduces the complexity of probabilistic models and the difficulty of the final interpretation of the aggregation from the workers' votes. We show improved accuracy against other WMV-based label aggregation strategies. We demonstrate the efficiency of our strategy on various real datasets to explore multiple crowdsourcing scenarios.
\medskip

\keywords{crowdsourcing, explainability, label aggregation, Shapley values}
\end{abstract}


\section{Introduction and related work}
\label{sec:intro}

Data annotation is a crucial step in the development of machine learning models. The quality of the annotations is a key factor in the performance of the models \citep{snow_cheap_2008}.
Frequently, the annotation process is outsourced to a crowd of non-expert workers through crowdsourcing platforms such as Amazon Mechanical Turk\footnote{\url{https://www.mturk.com/}}.
However, the quality of the annotations can vary greatly from one worker to another \citep{ross2009turkers,ipeirotis2010quality,hara2018data}.

To address this issue, several label aggregation strategies have been proposed in the literature.
The most common approach is the majority vote (MV) strategy, which consists of selecting the label with the largest number of responses.
While simple and easy to implement, MV has several limitations, such as not taking into account the reliability of the workers. Indeed, it affects the same weight to all the workers in the final aggregated label, no matter their level of expertise.

To alleviate this issue, probabilistic generative models such as DS \citep{dawid_maximum_1979} or GLAD \citep{whitehill_whose_2009} have been proposed, relying on generative models of the votes.
These models estimate the reliability of the workers and take it into account in the label aggregation process through different parameters. The DS model considers that each worker has an assigned confusion matrix -- to be estimated -- while GLAD models the reliability of the workers through a scalar weight and also includes the task's difficulty in the final aggregation.
Such a framework is flexible enough to incorporate various sources of information, and the inference is often based on the Expectation-Maximization algorithm which is computationally expensive and sensitive to initialization.
Moreover, the final interpretation of the aggregation from the workers' votes is not straightforward and the result of probabilistic models can be difficult to interpret for non-experts.

Weighted MV (WMV) strategies \citep{appen_wawa_2021,karger2011iterative,ma2020adversarial} have proven to be both effective and easy to interpret. Indeed, the method's principle is straightforward: each worker is assigned a weight that represents their reliability. The aggregated label is then the label that reflects the votes of workers relative to their reliability.

In this work, we aim to propose a new weight for WMV based on the Shapley values \citep{shapley1953value}.
The Shapley value is a concept originating from cooperative game theory that has been used in various fields such as economics \citep{aumann1994economic}, political science \citep{engelbrecht2009use}, and machine learning for explainability \citep{lundberg2017unified} or feature selection \citep{cohen2007feature}.
Shapley values have been used in the context of data valuation in classification \citep{schoch2022cs} and active learning \citep{ghorbani2022data}. Here, we propose to extend it to classification in a crowdsourcing setting.

If we consider that each worker is a feature and each task is a sample point, given a classifier, the Shapley value explains the contribution of each worker to the predicted outcome at each queried task \citep{molnar2020interpretable}.
Shapley values are used as worker importance indicators that can handle interactions between workers' answers \citep{owen2017shapley,lundberg2017unified}.
We propose a study of their usage as interpretable weights in weighted majority votes for crowdsourcing classification tasks.

\section{Notation and related work}

\paragraph{Notation.}
We consider classical multi-class learning notation, with input in $\mathcal{X}$ and labels in $[K]:=\{1,\dots,K\}$.
There are $n_\texttt{task}$ available, denoted $x_1,\dots,x_{n_\texttt{task}}$, to be labeled by $n_\texttt{worker}$ workers. The set of $n_\texttt{task}$ tasks with their associated true labels is $\mathcal{D}=\{(x_1,y_1^\star),\dots,(x_{n_\texttt{task}},y_{n_\texttt{task}}^\star)\}$.
Denote $\mathbf{Y}\in [K]^{n_\texttt{task}\times n_\texttt{worker}}$ the matrix of the workers' answers for each task.
The true labels are unobserved but crowdsourced labels are provided by the workers.
We write $\mathcal{A}(x_i)=\{j \in [n_\texttt{worker}]: \text{worker } j \text{ labeled task } x_i\}$ the \textbf{annotators set} of a task $x_i$.
For a task $x_i$ and each $j \in \mathcal{A}(x_i)$, we denote $\smash{y_i^{(j)}\in[K]}$ the label answered by worker $j$.
Given an aggregation strategy \texttt{agg} (such as MV), we denote aggregated label $\hat y^{\texttt{agg}}_i\in[K]$.
For any set $\mathcal{S}$, we write $|\mathcal{S}|$ for its cardinality.
The indicator function is denoted $\mathds{1}(\cdot)$.
The matrix full of ones of size $n\times m$ is denoted $\mathbf{1}_{n\times m}$.
The row of a matrix $M$ indexed by $i$ is denoted $M_{i,:}$ and the column indexed by $j$ is $M_{:,j}$.

On released datasets, to compute performance metrics, partial true labels are made available.
We denote $\mathcal{D}_\texttt{train}$ the set of tasks with their true labels unknown and $\mathcal{D}_\texttt{test}$ the set of tasks with known true labels. Note that these true labels are only used at test time.
Both workers and aggregation strategies do not have access to the true labels. Their goal is to recover it.

The impact of a worker on a task $x\in\mathcal{X}$, for a classifier $f$, is evaluated by a value function $\nu_{x,f}:2^{[n_\texttt{worker}]}\rightarrow \mathbb{R}$ such that for any set of workers $S\subset [n_\texttt{worker}]$ and any worker $j_0\notin S$, $\nu_{x,f}(S\cup\{j_0\}) - \nu(S)$ is the marginal contribution of worker $j_0$ over $S$. In a classification setting with output $f(x)$, the value function over a set $S\subseteq [n_\texttt{worker}]$ of workers is defined as
\begin{equation}\label{eq:nu}
  \nu_{x,f}(S) = \mathbb{E}[f(x)|x_S] \enspace.
\end{equation}
In practice, this quantity has to be estimated, for instance using the \texttt{TreeSHAP} algorithm \citep{lundberg2018consistent}.
\paragraph{Existing weighted label aggregation strategies.}

In this work, we focus on label aggregation strategies as weighted Majority Votes \citep{littlestone1994weighted} -- \emph{i.e.} that can be written as:
\begin{equation}
  \label{eq:wmv_general}
 \forall i \in [n_\texttt{task}], \quad \hat{y}_i = \text{WMV}(i, W):=\argmax_{k \in [K]} \sum_{j\in\mathcal{A}(x_i)} W_{j,k} \mathds{1}(y_i^{(j)} = k),
\end{equation}
where $W \in \mathbb{R}^{n_\texttt{worker}\times K}$ is the matrix assigning the weight of worker $j$ when answering class $k$.
We denote $W_{j,k}\in\mathbb{R}$ the weight of worker $j$ for class $k$.
This weight matrix is the cornerstone of each aggregation strategy.
% Note that since for any $\alpha \in \mathbb{R}$, $\text{WMV}(i, \alpha W) = \text{WMV}(i, W)$, we can assume that the weights are normalized such that $\sum_{j \in [n_\texttt{worker}]} \sum_{k=1}^K W_{j,k}=n_\texttt{worker}$ for all $j\in[n_\texttt{worker}]$.
% XXX: this corresponds to the ZBS choice. Actually it could be interesting to check the algorithm keeps this property. I doubt it...meaning that the weights are not normalized in the end. This might be a point to discuss.
% {XXX : $W_{j,k}\geq 0$ or not? to discuss}
We detail below popular label aggregation strategies that fit into this framework.
\begin{itemize}
  \item MV: The majority vote strategy assigns the label that has been chosen by the majority of the workers. It can be written as:
    \begin{equation}
      \label{eq:mv}
      \hat{y}_i^{\text{MV}} = \mathrm{WMV}(i,\mathbf{1}_{n_\texttt{worker}\times K})\enspace.
    \end{equation}
    This is the simplest weights assignment, where all workers and all labels share the same weight.
    \item WAWA \citep{appen_wawa_2021}: This strategy, also known as the inter-rater agreement, weights each user by how much they agree with the MV labels on average. More formally, given a task $i$:
    \begin{equation}
      \hat{y}_i^\text{WAWA}= \mathrm{WMV}\left(i,W\right),\quad
      \text{with}\quad W_{j,:} = \left(\frac{1}{|\{y_{i'}^{(j)}\}_{i'}|} \sum_{i'=1}^{n_{\mathrm{task}}} \mathds{1}\left(y_{i'}^{(j)} = \hat{y}_{i'}^\text{MV}\right)\right)\mathbf{1}_{K}\enspace.
    \end{equation}
    It allows us to instantiate a weight that can vary for each worker (but not per task) and it usually improves on the MV strategy.
    \item ZBS \citep{CrowdKit2023}: The Zero-Based Skill aggregation is a gradient descent (GD)-based version of the WAWA strategy.
    First, the labels are initialized using the MV strategy. Then, a descent step is performed on the weights to minimize the squared error between the current worker's weight and the weight assigned by the WAWA strategy. Finally, the aggregated labels are recomputed using the WMV strategy. This loop is repeated until convergence.
    \begin{algorithm}
      \caption{Zero Based Skill algorithm.}
      \begin{algorithmic}[1]
        \State \textbf{Input:} $\eta>0$ the learning rate, $t_{\max}>0$ maximum number of iterations,
      \State Initialize weights at step $0$: $W^0=\frac{1}{K} \mathbf{1}_{n_\texttt{worker}\times K}$.
        \For{$t=1,\dots, t_{\max}$}
            \State Update labels: $\hat{y}_i^{t} = \mathrm{WMV}(i, W^{t-1})$ for $i\in [n_\texttt{task}]$
            \State Compute current accuracy by worker: $a_j =\left(\frac{1}{|\{y_{i'}^{(j)}\}_{i'}|} \sum_{i'=1}^{n_{\mathrm{task}}} \mathds{1}\left(y_{i'}^{(j)} = \hat{y}_i^t\right)\right)\mathbf{1}_K$
            \State Update weights for each worker $j\in [n_\texttt{worker}]$: $W^t_{j,:} = W^{t-1}_{j, :} - \eta (W^{t-1}_{j,:}-a_j)$
        \EndFor
        \State \textbf{Output:} $\hat{y}_i^\text{ZBS} = \hat{y_i}^{t_{\max}}$.
      \end{algorithmic}
      \end{algorithm}

    % In the end, the last label's iterate is computed as a weighted majority vote from the last iterates of the estimated weights.

    \item WDS \citep{dawid_maximum_1979}:
    % XXX should we cite Maximilien here? (maximilien n'a pas fait de WDS)
    This strategy is based on the Dawid-Skene model. A confusion matrix $\pi^{(j)}\in\mathbb{R}^{K\times K}$ is associated to each worker, such that the $(k,\ell)$-entry or $\pi^{(j)}$ represents the probability for worker $j$ to answer $\ell\in[K]$ when the unknown true label is $k\in[K]$. For instance, each diagonal term represents the ability of the worker to answer correctly the underlying label.
  Using this DS diagonal, we obtain a weighted majority vote denoted WDS:
  \begin{equation}
    \hat{y}_i^\text{WDS}= \mathrm{WMV}\left(i, W\right),\quad
    \text{with}\quad W_{j,k} = \pi^{(j)}_{k,k}\enspace.
  \end{equation}
  \item M-MSR \citep{ma2020adversarial}: The Matrix Mean-Subsequence-Reduced strategy considers the reliability of all workers as a vector $s\in\mathbb{R}^{n_\texttt{worker}}$. Each entry $s_j$ represents the reliability of the worker $j$.
  This strategy assumes that each worker answers independently. It also assumes that a worker is correct with probability $p_j\in[0,1]$ and the worker's probability of being wrong is uniform across classes, \emph{i.e.}:
  \[\forall (i,j)\in [n_\texttt{task}]\times [n_\texttt{worker}],\ \begin{cases} \mathbb{P}(y_i^{(j)}=k)=p_j & \text{if } y_i^\star=k, \\ \mathbb{P}(y_i^{(j)}=k)=\frac{1-p_j}{K-1} & \text{if } y_i^\star\neq k\end{cases} \enspace.\]
  The reliability of a worker is linked to its probability of answering correctly: $s_j = \frac{K}{K-1}p_j - \frac{1}{K-1}$.
  This reliability can be estimated by solving a rank-one matrix completion problem defined as:
  \[\mathbb{E}\left[\frac{K}{K-1}C - \frac{1}{K-1}\mathds{1}\mathds{1}^\top\right]=ss^\top \enspace,\]
  where $C$ is the covariance matrix of the workers' answers.
  More precisely, given two workers $j,j' \in [n_\texttt{worker}]$, the covariance between them is $C_{j,j'} = \frac{1}{N_{j,j'}}\sum_{i=1}^{n_\texttt{task}} \mathds{1}(y_i^{(j)}=y_i^{(j')})$, with $N_{j,j'}$ the number of tasks in common: $N_{j,j'}=|\{i \in [n_\texttt{task}] |j,j'\in\mathcal{A}(x_i)\}|$. The final label is a weighted majority vote:
  \begin{equation}
    \hat y_i^{\text{M-MSR}} = \mathrm{WMV}(i, W)\quad \text{with}\quad W_{j,k}=\log\frac{(K-1)p_j}{1-p_j}\enspace,
  \end{equation}
  where the form of the weights is derived from a maximum a posteriori formulation of the model, see \citep[Corollary 9]{li2014error}.
  % XXX Expectation C, and the model itself are a bit obscure, we should detail this more.

  % XXX : we should drop this strategy, or reduce it drastically as it is not adapted to non-binary settings.
  \item KOS \citep{karger2011iterative}: Only set for binary classification $K=2$ ($y_i^\star\in\{\pm 1\}$) the KOS strategy from a graph-theory perspective.
  The KOS strategy estimates the reliability of a worker based on the consistency of their answers with those of their neighbors and the likelihood of a task having $y_i^\star=1$.
  % Denote $G([n_\texttt{task}]\cup [n_\texttt{worker}], E)$ the graph where edge $(i,j)$ is connected if worker $j$ has answered task $i$. The neighborhood of a task $i$ is denoted $\partial_i$ and the same with index $j$ for workers.
  % On each edge is stored the label answered by worker $j$ for task $i$: $y_i^{(j)}\in\{-1,1\}$.
  % The KOS strategy estimates the reliability of a worker based on the consistency of their answers with those of their neighbors and the likelihood of a task having $y_i^\star=1$.
  % Both pieces of information are propagated into the graph as messages.
  % The final label is then estimated by the sign of the sum of the messages received by the task weighted by the workers' reliability.
  The neighborhood of a task $i$ is denoted $\partial_i$ and the same with index $j$ for workers. A task message denoted $x_{i\rightarrow j}\in\mathbb{R}$ is the log-likelihood of the task $i$ having $y_i^\star=1$.
  A worker message denoted $y_{j\rightarrow i}\in\mathbb{R}$ is the reliability of worker $j$ for task $i$.
  % Both messages are real values.
  The messages are updated iteratively until convergence following the equations:
  \begin{equation*}
      x_{i\rightarrow j} \gets \sum_{j'\in \partial_i\setminus \{j\}} y_i^{(j')}y_{j'\rightarrow i} \quad \text{and}\quad y_{j\rightarrow i} \gets \sum_{i'\in \partial_j\setminus \{i\}} y_{i'}^{(j)} x_{i'\rightarrow j} \enspace.
  \end{equation*}
  Finally, the estimated label is computed as:
  \begin{equation}
    \hat y_i^{\text{KOS}} = \mathrm{sign}\left(\mathrm{WMV}(i, \{W_i\}_{i\in [n_\texttt{task}]} )\right)\quad \text{with}\quad (W_i)_{j,:}=\sum_{j\in\partial_i}y_{j\rightarrow i}\mathbf{1}_{K}\enspace.
  \end{equation}
  The worker's weight is estimated iteratively inspired by the belief propagation algorithm to look at the worker agreements on neighboring tasks.
  Note that the KOS strategy is based on a weighted majority vote -- hence why we included it -- but the weights take into account the task's index, thus leading to $n_\texttt{task}$ weight matrices.

\end{itemize}
Note that depending on the strategy, the weights $W_{j,k}$ might not be upper-bounded. Indeed, in WAWA as the weight is the accuracy, it is at most $1$. However, KOS strategy does not have an upper bound on the weights.
If the weights are not upper-bounded, the more a worker answers following other workers, the more weight they will accumulate. This kind of strategy rewards engagement.
In \Cref{fig:weights_by_strat} we show how each strategy leads to different weights and scales. The weights are computed for the BlueBirds dataset \citep{WelinderEtal10b} presented in more detail in \Cref{sec:results}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{../matrix_weights_bluebirds.pdf}
  \caption{Example of weight matrices $W\in\mathbb{R}^{n_\texttt{worker}\times K}$ obtained for the BlueBirds ($n_\texttt{worker}=39, K=2$) dataset using the presented strategies and our in \Cref{alg:shap}. We transpose them for ease of readability (each row is a class, each column a worker). Only WDS takes into account the class label. For KOS, we show the weight in absolute value, averaged over the tasks (as the weights are task-dependent). However, this transformation does not affect the scale of magnitude of weights.}
  \label{fig:weights_by_strat}
\end{figure}

\section{Shapley label aggregation for crowdsourcing}
\label{sec:shapagg}

\subsection{Preliminaries on Shapley values}

Shapley values have been used to quantify the contribution of individual features in machine learning models' prediction \citep{molnar2020interpretable}.
In the context of crowdsourcing, we propose to use Shapley values to quantify the contribution of each worker to the final label aggregation.

% XXX might be easier to instantiate in your context with: $T=[n_\texttt{worker}]$. One letter spared. and T could be silent.
% Done
% XXX Instantiate here  the $\nu$ function we are going to use next "-- for example the prediction score of a classifier -- " so you can silence the $\nu$ in the definition of the Shapley value.
% XXX why not only using $\phi_j(i)$ since $T$ and $\nu$ will be fixed

\begin{definition}
Given a set of $n_\texttt{worker}$ workers, a classifier $f$, a task $x_i\in\mathcal{X}$ and a value function $\nu_{x_i, f}(\cdot): 2^{[n_\texttt{worker}]}\rightarrow \mathbb{R}$ the Shapley value of a worker $j$ for a task $i$ is defined as the average marginal contribution of the worker $j$ to every subset of $[n_\texttt{worker}]\setminus\{j\}$:
\begin{equation}
  \phi_j(i, \nu) = \sum_{S\subseteq [n_\texttt{worker}]\setminus\{j\}} \frac{|S|!(n-|S|-1)!}{n!} \left[\nu_{x_i, f}(S\cup\{j\}) - \nu_{x_i, f}(S)\right]\enspace.
\end{equation}
When there is no ambiguity over the value function, we adopt the standard notation abuse $\phi_j(i, \nu)=\phi_j(i)$.
\end{definition}

Shapley values satisfy the following properties -- given a task $x$ and a classifier $f$:
\begin{itemize}
  \item Symmetry: if $\nu_{x,f}(S\cup\{p\})=\nu_{x,f}(S\cup\{q\})$ for all set $S\subseteq [n_\texttt{worker}]\setminus\{p,q\}$, then $\phi_p=\phi_q$.
  \item Null worker: if $\nu_{x,f}(S\cup\{p\})=\nu_{x,f}(S)$ for $S\subseteq [n_\texttt{worker}]$ then $\phi_p=0$.
  \item Additivity: for two value functions $\nu^1_{x,f}$ and $\nu^2_{x,f}$, $\phi_j(i,\nu^1_{x,f}+\nu^2_{x,f})=\phi_j(i, \nu^1_{x,f})+\phi_j(i, \nu^2_{x,f})$.
  \item Efficiency: $\sum_{j=1}^{n_\texttt{worker}} \phi_j(\nu)=\nu_{x,f}([n_\texttt{worker}])$.
\end{itemize}


Where the Shapley value is interesting for a crowdsourcing problem, is that if a worker does not help the classifier to predict the label, then its Shapley value will be close to zero. And, two workers with similar contributions will obtain similar Shapley values.
\subsection{Shapley label aggregation strategy}

We introduce the following label aggregation algorithm based on Shapley values.
It is based on the Expectation-Maximization procedure where we iteratively estimate the labels and the workers' skills until convergence -- \emph{e.g.} stabilization of the skills.
Given a current estimation of the labels and a classifier $f$, we consider the skill of each worker as their total contribution to the prediction.
The contribution of a worker $j_0\in[n_\texttt{worker}]$ on a single task $i_0\in [n_\texttt{task}]$ is given as $|\phi_{j_0}(i_0)|\in\mathbb{R}_+$.

\begin{algorithm}[tbh]
  \caption{Shapley label aggregation strategy.}\label{alg:shap}
  \begin{algorithmic}[1]
    \State \textbf{Input:} classifier $f$, $t_{\max}>0$ maximum number of iterations
  \State Initialize labels with MV: $\hat{y}_i^0 = \mathrm{WMV}(i, \mathbf{1}_{n_\texttt{worker}\times K})$ for each task $i\in [n_\texttt{task}]$
    \For{$t=0,\dots, t_{\max} - 1 $}
        \State Train classifier $f$ on $\{(\mathbf{Y}, \hat{y}_i^t)_{i\in [n_\texttt{task}]}\}$ (workers' answers and current aggregated labels)
        \State Compute Shapley values' total contribution of each worker $j\in [n_\texttt{worker}]$:
        \[ \mathrm{contrib}(j) = \sum_{i=1}^{n_\texttt{task}} |\phi_{j}(i)|\enspace. \]
        \State Update weights: $W^t_{j,:} = \mathrm{contrib}(j)\mathbf{1}_K$ for each worker $j\in [n_\texttt{worker}]$
        \State Update labels with WMV: $\hat{y}_i^{t+1} = \mathrm{WMV}(i, W^{t})$ for $i\in[n_\texttt{task}]$.
    \EndFor
    \State \textbf{Output:} $\hat{y}_i^\text{shapley} = \hat{y_i}^{t_{\max}}$.
  \end{algorithmic}
  \end{algorithm}

First, note that in \Cref{alg:shap} the participation of each worker is linked to their total contribution. There is no upper bound on the skill estimation with $\mathrm{contrib}$ as we value a worker who answers multiple times.
However, if they answer many labels randomly, their Shapley value is close to zero and their total contribution is low.
As it can be seen in \Cref{fig:weights_by_strat}, as the Shapley values can be used for feature importance in prediction, it can also identify which workers are the most important for the final label aggregation and could be the center of more analysis.
And, at the same time, it can identify which workers are not contributing to the final label.

\subsection{Implementation}
\label{sub:implementation}

To compute Shapley values, we use the \texttt{Shap} library \citep{lundberg2017unified}.
We choose an XGBOOST classifier \citep{chen2016xgboost} as the classifier $f$.
In practice, the value function $\nu_{x,f}$ evaluated at a set $S\subseteq [n_\texttt{worker}]$ defined in \Cref{eq:nu} is estimated using the \texttt{TreeSHAP} algorithm \citep{lundberg2018consistent}.
Label aggregation strategies are implemented in Python using the \texttt{crowd-kit} \citep{CrowdKit2023} or \texttt{peerannot} \citep{peerannot} libraries.
The XGBOOST classifier is known to have an extensive number of hyperparameters to tune.
To choose them, we first use the \texttt{optuna} \citep{optuna} library to tune over a $3$-fold cross-validation of best hyperparameters for the set of tasks and label $\{(x_i, \hat{y}_i^0)_{i\in [n_\texttt{task}]}\}$.
This random search includes the trees' depth, learning rate, the number of trees, the minimum child weight, the subsampling proportion and regularization parameter. These best parameters are then used in \Cref{alg:shap} to iteratively train the XGBOOST model with the current label estimates. Note that this hyperparameter search can be costly in computation time.

\subsection{Evaluation metrics}

We evaluate the performance of the Shapley label aggregation strategy using the accuracy and the F1 score.
More precisely, each of the real datasets considered provides a -- partially known -- ground truth.
This test set is denoted $\mathcal{D}_\texttt{test}=\{(x_i, y_i^\star)\}_{i=1}^{n_\texttt{test}}$ and is used to evaluate the accuracy of the label aggregation strategies.
This ground truth is not used during the aggregation, only at evaluation time.
The accuracy of the aggregation strategy \texttt{agg} is the proportion of correctly predicted labels $(\hat[y]^\texttt{agg}_i)_{i=1}^{n_\texttt{test}}$ over the total number of tasks in $\mathcal{D}_\texttt{test}$ with ground truth in $y^\star=(y_i^\star)_{i=1}^{n_\texttt{test}}$:
\[\mathrm{Accuracy}(\hat{y}^\texttt{agg}, y^\star) = \frac{1}{n_\texttt{test}}\sum_{i=1}^{n_\texttt{test}} \mathds{1}(\hat{y}^\texttt{agg}_i=y_i^\star) \enspace.\]
We take into account the possible class imbalance by presenting a macro-average F1 score. This score is commonly used to evaluate the balance between precision and recall in classification tasks. It provides a measure of the quality of the label aggregation strategy when dealing with imbalanced datasets.
Denoting respectively $\mathrm{TP}_k$, $\mathrm{FP}_k$ and $\mathrm{FN}_k$ the true positives, false positives and false negatives related to the class $k\in[K]$, the macro averaged F1-score writes
\[
\mathrm{F1} = \frac{1}{K}\sum_{k=1}^K\frac{\mathrm{TP}_k}{\mathrm{TP}_k + 0.5(\mathrm{FN}_k + \mathrm{FP}_k)}\enspace.
\]

These metrics are evaluated on several real datasets.
The BlueBirds dataset \citep{WelinderEtal10b} is a binary classification ($K=2$) dataset with $n_\texttt{task}=108$ tasks and $n_\texttt{worker}=39$ workers. Workers were asked to identify if there was a blue bird of the species Indigo Bunting in the presented image.
The Temporal Ordering (Temp) \citep{snow_cheap_2008} dataset is a binary classification dataset with $n_\texttt{task}=462$ tasks and $n_\texttt{worker}=76$ workers. Workers were presented with sentences with events and asked if the event presented in the first sentence occurred before the one in the second sentence.
The LabelMe dataset \citep{rodrigues2018deep} consists of $n_\texttt{task}=1000$ images shown to $n_\texttt{worker}=77$ workers. The task was to classify the image into one of the $K=8$ classes.
Finally, the Music dataset \citep{rodrigues2014gaussian} is a music genre classification for $n_\texttt{task}=700$ samples annotated by $n_\texttt{worker}=44$ workers. There are $K=10$ different music genres to be assigned to each task.

\section{Results}
\label{sec:results}
\subsection{Performance on real datasets}

We evaluate the Shapley label aggregation strategy on several real datasets.
From \Cref{tab:scores}, we see that using Shapley-based weights in the WMV strategy outperforms other strategies in terms of accuracy and F1 score.
Note that the KOS strategy can not be applied to the datasets considered with $K>2$ as it is only suited for binary classification tasks.

\begin{table}[h]
  \centering
  \caption{Accuracy and F1 Score of the WMV-based label aggregation strategies over 4 real datasets: BlueBirds, Temp, LabelMe and Music. We obtain equal or better performance in accuracy and F1 score for 3 out of the 4 datasets.\\ }
  \label{tab:scores}

  \footnotesize{
  \begin{tabular}{l|cc|cc|cc|cc}
  \hline
  \multirow{2}{*}{\textbf{Strategy}} & \multicolumn{2}{c|}{\textbf{BlueBirds} $(K=2)$} & \multicolumn{2}{c|}{\textbf{Temp} $(K=2)$} & \multicolumn{2}{c|}{\textbf{LabelMe} $(K=8)$} & \multicolumn{2}{c}{\textbf{Music} $(K=10)$} \\
  \cline{2-9}
   & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{Accuracy} & \textbf{F1 Score} \\
  \hline
  MV    & 0.759 & 0.742 & 0.569 & 0.385 & 0.769 & 0.765 & 0.711 & 0.744 \\
  WAWA  & 0.759 & 0.742 & \textbf{0.945} & \textbf{0.945} & 0.770 & 0.766 & 0.797 & 0.801 \\
  ZBS   & 0.648 & 0.623 & 0.943 & 0.943 & 0.774 & 0.769 & \textbf{0.800} & \textbf{0.804} \\
  WDS   & 0.759 & 0.736 & \textbf{0.945} & \textbf{0.945} & 0.736 & 0.724 & 0.794 & 0.797 \\
  KOS   & 0.722 & 0.678 & 0.569 & 0.384 & --- & --- & --- & --- \\
  M-MSR & 0.639 & 0.578 & 0.924 & 0.922 & 0.767 & 0.761 & 0.742 & 0.744 \\
  \rowcolor{lightgray} Shapley & \textbf{0.805}& \textbf{0.794}& \textbf{0.945} & \textbf{0.945} & \textbf{0.777} & \textbf{0.762} & 0.760 & 0.765  \\

  \hline
  \end{tabular}
  }
  \end{table}
% \subsection{Performance varying the number of workers}

% From the real datasets, we can explore how the number of workers impacts the performance of the different label aggregation strategies.
% To do so, given a dataset, we randomly choose the same subset of $5$ workers for the label aggregation strategies.
% Then, we add workers one by one, chosen uniformly at random, and evaluate the accuracy and F1 score of the label aggregation strategies with the new set of workers.

\subsection{More information on workers}

\begin{figure}[htb]
  \centering
  \includegraphics[width=.9\textwidth]{./../summary_plot_shap_all_bluebirds.pdf}
  \caption{Summary of the main contributing workers for the BlueBirds dataset using Shapley values. Left: Worker $0$ has the highest contribution to the final label -- either for class $0$ or $1$, followed by worker $15$. Then, worker $36$ has better identification for class $1$ than class $0$. Right: Average impact of each worker on the final predicted label by the XGBOOST classifier. This worker's contribution scalar value -- used in \Cref{alg:shap} weight $W_{j,:}$ -- does not allow to differentiate between classes.}
  \label{fig:shap_bluebirds}
\end{figure}

Using the Shapley values as workers' contribution, we can also provide more information on the workers' reliability.
Let us explore the BlueBirds' dataset Shapley weights as an example.
From \Cref{fig:shap_bluebirds}, we see that worker $0$ has the best overall contribution to the final label.
Note that this order of contribution given by Shapley values is in agreement with the accuracy of the workers even though Shapley values are not directly linked to the accuracy of a model.
Indeed, as we know the ground truth, we can compute the accuracy of each worker.
The accuracy of worker $0$ is $0.80$, worker $15$ is $0.73$ -- the two main workers contributing to the final aggregation -- and worker $10$ is $0.5$ (random answers).
The worker $22$ -- not represented in \Cref{fig:shap_bluebirds} as they are not a main contributor -- has an accuracy of $0.42$ -- worse than a random guess -- and an average absolute contribution of $0.03$ to the final label. This worker is indeed not contributing to the final label given the poor quality of their answers.

Worker $15$ is the second-best contributor, and worker $36$ has a better identification for class $1$ than class $0$.
However, as we use a single scalar value that is class-blind in \Cref{alg:shap} to aggregate the label in the WMV, this asymmetrical contribution is not taken into account.
This is a limitation of the current Shapley label aggregation strategy.


\section{Conclusion}
\label{sec:conclusion}

We introduced a new label aggregation strategy based on Shapley values for crowdsourcing classification tasks.
In the framework of weighted majority votes, we used the Shapley values as workers' weights to aggregate the labels.
We showed that this strategy outperforms other weighted majority vote strategies on real datasets in terms of accuracy and F1 score.
Moreover, we discussed how Shapley-based skills can be used to explore workers' reliability.
However, this strategy is limited by the scalar value used to aggregate the labels in the WMV strategy.
Not unlike most other WMV strategies, it does not take into account per-class skills.
An extension of this work would be to consider multidimensional skills based on Shapley values for each worker, allowing for a per-class contribution to the final label and a finer estimation of workers' skills.

\bibliographystyle{apalike}
\bibliography{cap2024}

\end{document}